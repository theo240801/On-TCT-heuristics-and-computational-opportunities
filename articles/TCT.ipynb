{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des librairies nécessaires__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__définition des hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_client': 5,\n",
    "    'seed': 123,\n",
    "    'num_samples_per_client': 500,\n",
    "    'rounds_stage1': 30, #100 de base\n",
    "    'local_epochs_stage1': 5,\n",
    "    'mini_batchsize_stage1': 64,\n",
    "    'local_lr_stage1': 0.1,\n",
    "    'rounds_stage2': 30, #100 de base\n",
    "    'local_steps_stage2': 100,\n",
    "    'local_lr_stage2': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__création d'un dossier de sauvegarde pour les modèles successifs du stage 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\dmgtr\\\\OneDrive - Ecole Polytechnique\\\\3A\\\\P1\\\\MAP578 - EA collaborative learning\\\\Project\\\\TCT')\n",
    "\n",
    "isExist = os.path.exists('./ckpt_stage1')\n",
    "if not isExist:\n",
    "   os.makedirs('./ckpt_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = args[\"num_client\"]\n",
    "num_rounds_stage1 = args[\"rounds_stage1\"]\n",
    "epochs_stage1 = args[\"local_epochs_stage1\"]\n",
    "batch_size_stage1 = args[\"mini_batchsize_stage1\"]\n",
    "lr_stage1 = args[\"local_lr_stage1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création des datasets décentralisés (ie non idd)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load les data MNIST, les transformer en tensor et les normaliser\n",
    "traindata = datasets.MNIST('./data_mnist', train=True, download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)]) # 10 x 60000 (one-hot qui détermine la label correpondant à la ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_split = []\n",
    "torch.manual_seed(args[\"seed\"]) # pour que les splits soient les mêmes à chaque fois\n",
    "torch.cuda.manual_seed(args[\"seed\"])  # pour que les splits soient les mêmes à chaque fois\n",
    "\n",
    "for i in range(num_clients):\n",
    "    index_split = torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0] # on prend les labels 2i et 2i+1\n",
    "    perm_split = torch.randperm(index_split.size(0)) # on mélange les indices\n",
    "    index_split_subsample = index_split[perm_split[:args[\"num_samples_per_client\"]]] # on prend les 500 premiers\n",
    "    target_labels_split += [index_split_subsample] # on ajoute à la liste des labels splités\n",
    "\n",
    "#Chacun des 5 clients reçoit 500 images d'un des deux labels associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets (subsampled)\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split] # chaque élément contient les images et labels d'un client\n",
    "train_loader = [torch.utils.data.DataLoader(train_subset, batch_size=batch_size_stage1, shuffle=True)\n",
    "                for train_subset in traindata_split] # on crée les dataloader associés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création du dataset global de test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (subsampled)\n",
    "testdata = datasets.MNIST('./data_mnist', train=False,\n",
    "                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                        transforms.Normalize((0.1307,), (0.3081,))])) # on charge les données de test\n",
    "\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "torch.cuda.manual_seed(args[\"seed\"])\n",
    "perm_split_test = torch.randperm(testdata.targets.shape[0])\n",
    "testdata_subset = torch.utils.data.Subset(testdata, perm_split_test[:1000])\n",
    "test_loader = torch.utils.data.DataLoader(testdata_subset, batch_size=batch_size_stage1, shuffle=False) #pas de shuffle pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle de réseau de neurones de base__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ajouté depuis utils\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net() #modifié depuis Net().cuda() #Modèle fédéré\n",
    "client_models = [Net() for _ in range(num_clients)] #modifié depuis Net().cuda() #Modèles des clients\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "opt = [optim.SGD(model.parameters(), lr=lr_stage1) for model in client_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __FedAvg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client_model, optimizer, train_loader, epoch=5): #réalise un pas d'optimization et retourne la loss\n",
    "    \"\"\"Train a client_model on the train_loder data.\"\"\"\n",
    "    client_model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data, target #supprimer data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_models(global_model, client_models):\n",
    "    \"\"\"Average models across all clients.\"\"\"\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0) #stack les résultas et fait la moyenne par client\n",
    "    global_model.load_state_dict(global_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Compute loss and accuracy of a single model on a data_loader.\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data, target #supprimer .cuda()\n",
    "            output = model(data)\n",
    "            loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(data_loader.dataset)\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "def evaluate_many_models(models, data_loader):\n",
    "    \"\"\"Compute average loss and accuracy of multiple models on a data_loader.\"\"\"\n",
    "    num_nodes = len(models)\n",
    "    losses = np.zeros(num_nodes)\n",
    "    accuracies = np.zeros(num_nodes)\n",
    "    for i in range(num_nodes):\n",
    "        losses[i], accuracies[i] = evaluate_model(models[i], data_loader)\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round: average train loss 0.168 | average test loss 6.74 | average test acc: 0.194\n",
      "1-th round: average train loss 0.0942 | average test loss 7.74 | average test acc: 0.191\n",
      "2-th round: average train loss 0.0442 | average test loss 7.57 | average test acc: 0.195\n",
      "3-th round: average train loss 0.0277 | average test loss 7.24 | average test acc: 0.195\n",
      "4-th round: average train loss 0.0269 | average test loss 7.62 | average test acc: 0.196\n",
      "5-th round: average train loss 0.0244 | average test loss 6.69 | average test acc: 0.193\n",
      "6-th round: average train loss 0.0285 | average test loss 6.71 | average test acc: 0.196\n",
      "7-th round: average train loss 0.0298 | average test loss 6.09 | average test acc: 0.197\n",
      "8-th round: average train loss 0.0139 | average test loss 6.04 | average test acc: 0.205\n",
      "9-th round: average train loss 0.0122 | average test loss 5.79 | average test acc: 0.204\n",
      "10-th round: average train loss 0.0116 | average test loss 5.37 | average test acc: 0.222\n",
      "11-th round: average train loss 0.00315 | average test loss 5.48 | average test acc: 0.238\n",
      "12-th round: average train loss 0.00221 | average test loss 5.91 | average test acc: 0.221\n",
      "13-th round: average train loss 0.00475 | average test loss 5.5 | average test acc: 0.237\n",
      "14-th round: average train loss 0.00399 | average test loss 5.39 | average test acc: 0.241\n",
      "15-th round: average train loss 0.00153 | average test loss 5.22 | average test acc: 0.258\n",
      "16-th round: average train loss 0.00195 | average test loss 5.55 | average test acc: 0.241\n",
      "17-th round: average train loss 0.00381 | average test loss 5.1 | average test acc: 0.276\n",
      "18-th round: average train loss 0.00197 | average test loss 5.03 | average test acc: 0.254\n",
      "19-th round: average train loss 0.0125 | average test loss 4.18 | average test acc: 0.312\n",
      "20-th round: average train loss 0.0025 | average test loss 3.98 | average test acc: 0.280\n",
      "21-th round: average train loss 0.00247 | average test loss 4.51 | average test acc: 0.305\n",
      "22-th round: average train loss 0.00367 | average test loss 3.91 | average test acc: 0.306\n",
      "23-th round: average train loss 0.00216 | average test loss 3.81 | average test acc: 0.313\n",
      "24-th round: average train loss 0.000747 | average test loss 3.8 | average test acc: 0.324\n",
      "25-th round: average train loss 0.00119 | average test loss 3.64 | average test acc: 0.354\n",
      "26-th round: average train loss 0.000908 | average test loss 3.42 | average test acc: 0.373\n",
      "27-th round: average train loss 0.00117 | average test loss 4.1 | average test acc: 0.374\n",
      "28-th round: average train loss 0.000715 | average test loss 3.58 | average test acc: 0.384\n",
      "29-th round: average train loss 0.00105 | average test loss 4.83 | average test acc: 0.363\n"
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage1 (i.e., FedAvg)\n",
    "for r in range(num_rounds_stage1):\n",
    "    # load global weights\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients):\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[i], epoch=epochs_stage1)\n",
    "\n",
    "    # average params across neighbors\n",
    "    average_models(global_model, client_models)\n",
    "\n",
    "    # evaluate\n",
    "    test_losses, accuracies = evaluate_many_models(client_models, test_loader)\n",
    "    torch.save(client_models[0].state_dict(), './ckpt_stage1/model_tct_stage1.pth')\n",
    "\n",
    "    print('%d-th round: average train loss %0.3g | average test loss %0.3g | average test acc: %0.3f' % (\n",
    "    r, loss / num_clients, test_losses.mean(), accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_stage2 = args[\"rounds_stage2\"]\n",
    "batch_size = args[\"num_samples_per_client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_eNTK(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_eNTK, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "# Init and load model ckpt\n",
    "global_model = Net_eNTK() #supprimer .cuda()\n",
    "global_model.load_state_dict(torch.load('./ckpt_stage1/model_tct_stage1.pth'))\n",
    "global_model.fc2 = nn.Linear(128, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "print('load model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Compute eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eNTK(model, X, subsample_size=100000, seed=123):\n",
    "    \"\"\"\"compute eNTK\"\"\"\n",
    "    model.eval()\n",
    "    params = list(model.parameters()) #liste de tous les paramètres trainable du modèle\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random_index = torch.randperm(355073)[:subsample_size]\n",
    "    grads = None\n",
    "    for i in tqdm(range(X.size()[0])):\n",
    "        model.zero_grad() #réinitialise les gradients\n",
    "        model.forward(X[i : i+1])[0].backward() #calcule les gradients  \n",
    "\n",
    "        grad = []\n",
    "        for param in params: #param.requires_grad dans PyTorch est un attribut des tenseurs (y compris ceux qui représentent les paramètres d'un modèle, comme les poids et les biais d'un réseau de neurones) qui détermine si PyTorch doit calculer les gradients pour ces tenseurs pendant la rétropropagation.\n",
    "            if param.requires_grad:\n",
    "                grad.append(param.grad.flatten())\n",
    "        grad = torch.cat(grad) #Concatène tous les gradients aplatis en un seul vecteur\n",
    "        grad = grad[random_index] #Réduit la dimensionnalité du vecteur de gradient en utilisant l'indexation aléatoire définie par random_index\n",
    "\n",
    "        if grads is None:\n",
    "            grads = torch.zeros((X.size()[0], grad.size()[0]), dtype=torch.half) #Si grads est None, une matrice zéro de la forme appropriée est initialisée.\n",
    "        grads[i, :] = grad #Stocke le vecteur de gradient subsamplé pour l'échantillon i dans la matrice grads\n",
    "\n",
    "    return grads\n",
    "\n",
    "def client_compute_eNTK(client_model, train_loader):\n",
    "    \"\"\"Train a client_model on the train_loder data.\"\"\"\n",
    "    client_model.train()\n",
    "\n",
    "    data, targets = next(iter(train_loader))\n",
    "    grads_data = compute_eNTK(client_model, data) #supprimer .cuda()\n",
    "    grads_data = grads_data.float() #supprimer .cuda()\n",
    "    targets = targets #supprimer .cuda()\n",
    "    # gradient\n",
    "    targets_onehot = F.one_hot(targets, num_classes=10) - (1.0 / 10.0) #supprimer .cuda()\n",
    "    del data\n",
    "    torch.cuda.empty_cache()\n",
    "    return grads_data, targets_onehot, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 265.21it/s]\n",
      " 44%|████▍     | 28/64 [00:00<00:00, 277.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 210.86it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 379.58it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 363.16it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 310.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_all = []\n",
    "target_all = []\n",
    "target_onehot_all = []\n",
    "for i in range(num_clients):\n",
    "    grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i])\n",
    "    grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "    target_all.append(copy.deepcopy(target_i).cpu())\n",
    "    target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "    del grad_i\n",
    "    del target_onehot_i\n",
    "    del target_i\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100000])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 120.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "grad_eval, target_eval_onehot, target_eval  = client_compute_eNTK(global_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __run stage 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmgtr\\AppData\\Local\\Temp\\ipykernel_19292\\1872500005.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta_global = torch.tensor(theta_global, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "# Init linear models\n",
    "theta_global = torch.zeros(100000, 10) #supprimer .cuda()\n",
    "theta_global = torch.tensor(theta_global, requires_grad=False)\n",
    "client_thetas = [torch.zeros_like(theta_global) for _ in range(num_clients)] #supprimer .cuda()\n",
    "client_hi_s = [torch.zeros_like(theta_global) for _ in range(num_clients)] #supprimer .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_update(grads_data, targets, theta_client, h_i_client_pre, theta_global,\n",
    "                    M=200, lr_local=0.00001):\n",
    "    # set up data / eNTK\n",
    "    grads_data = grads_data.float() #supprimer .cuda()\n",
    "    targets = targets #supprimer .cuda()\n",
    "\n",
    "    # compute transformed onehot label\n",
    "    targets_onehot = F.one_hot(targets, num_classes=10) - (1.0 / 10.0) #supprimer .cuda()\n",
    "    num_samples = targets_onehot.shape[0]\n",
    "\n",
    "    # compute updates\n",
    "    h_i_client_update = h_i_client_pre + (1 / (M * lr_local)) * (theta_global - theta_client)\n",
    "    theta_hat_local = (theta_global) * 1.0\n",
    "\n",
    "    # local gd\n",
    "    for local_iter in range(M):\n",
    "        theta_hat_local -= lr_local * ((1.0 / num_samples) * grads_data.t() @ (grads_data @ theta_hat_local - targets_onehot) - h_i_client_update)\n",
    "\n",
    "    del targets\n",
    "    del grads_data\n",
    "    torch.cuda.empty_cache()\n",
    "    return theta_hat_local, h_i_client_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: train accuracy=0.79062 test accuracy=0.78125\n",
      "Round 1: train accuracy=0.83125 test accuracy=0.85938\n",
      "Round 2: train accuracy=0.86563 test accuracy=0.875\n",
      "Round 3: train accuracy=0.88125 test accuracy=0.875\n",
      "Round 4: train accuracy=0.90312 test accuracy=0.875\n",
      "Round 5: train accuracy=0.92188 test accuracy=0.85938\n",
      "Round 6: train accuracy=0.93437 test accuracy=0.85938\n",
      "Round 7: train accuracy=0.94375 test accuracy=0.875\n",
      "Round 8: train accuracy=0.95 test accuracy=0.875\n",
      "Round 9: train accuracy=0.95938 test accuracy=0.875\n",
      "Round 10: train accuracy=0.96562 test accuracy=0.875\n",
      "Round 11: train accuracy=0.96875 test accuracy=0.875\n",
      "Round 12: train accuracy=0.96875 test accuracy=0.89062\n",
      "Round 13: train accuracy=0.96875 test accuracy=0.90625\n",
      "Round 14: train accuracy=0.97188 test accuracy=0.90625\n",
      "Round 15: train accuracy=0.975 test accuracy=0.92188\n",
      "Round 16: train accuracy=0.975 test accuracy=0.92188\n",
      "Round 17: train accuracy=0.97812 test accuracy=0.92188\n",
      "Round 18: train accuracy=0.97812 test accuracy=0.92188\n",
      "Round 19: train accuracy=0.97812 test accuracy=0.9375\n",
      "Round 20: train accuracy=0.97812 test accuracy=0.9375\n",
      "Round 21: train accuracy=0.97812 test accuracy=0.9375\n",
      "Round 22: train accuracy=0.97812 test accuracy=0.9375\n",
      "Round 23: train accuracy=0.98125 test accuracy=0.9375\n",
      "Round 24: train accuracy=0.98438 test accuracy=0.9375\n",
      "Round 25: train accuracy=0.9875 test accuracy=0.9375\n",
      "Round 26: train accuracy=0.9875 test accuracy=0.9375\n",
      "Round 27: train accuracy=0.9875 test accuracy=0.9375\n",
      "Round 28: train accuracy=0.9875 test accuracy=0.9375\n",
      "Round 29: train accuracy=0.9875 test accuracy=0.9375\n"
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage2\n",
    "for round_idx in range(num_rounds_stage2):\n",
    "    theta_list = []\n",
    "    for i in range(num_clients):\n",
    "        theta_hat_update, h_i_client_update = scaffold_update(grad_all[i],\n",
    "                                                              target_all[i],\n",
    "                                                              client_thetas[i],\n",
    "                                                              client_hi_s[i],\n",
    "                                                              theta_global,\n",
    "                                                              M=args[\"local_steps_stage2\"],\n",
    "                                                              lr_local=args[\"local_lr_stage2\"])\n",
    "        client_hi_s[i] = h_i_client_update * 1.0\n",
    "        client_thetas[i] = theta_hat_update * 1.0\n",
    "        theta_list.append(theta_hat_update)\n",
    "\n",
    "    # averaging\n",
    "    theta_global = torch.zeros_like(theta_list[0]) #supprimer .cuda()\n",
    "    for theta_idx in range(num_clients):\n",
    "        theta_global += (1.0 / num_clients) * theta_list[theta_idx]\n",
    "\n",
    "    # eval on train\n",
    "    logits_class_train = torch.cat(grad_all) @ theta_global #supprimer .cuda()\n",
    "    _, targets_pred_train = logits_class_train.max(1)\n",
    "    train_acc = targets_pred_train.eq(torch.cat(target_all)).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "    # eval on test\n",
    "    logits_class_test = grad_eval @ theta_global\n",
    "    _, targets_pred_test = logits_class_test.max(1)\n",
    "    test_acc = targets_pred_test.eq(target_eval).sum() / (1.0 * logits_class_test.shape[0]) #supprimer .cuda()\n",
    "    print('Round %d: train accuracy=%0.5g test accuracy=%0.5g' % (round_idx, train_acc.item(), test_acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
