{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des librairies nécessaires__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des autres fichiers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\MAP\\chocoEA\\impl\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\MAP\\chocoEA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_orga.Models import Net, Net_eNTK, ResNet18, ResNet14\n",
    "from nv_orga.FedAvg import average_models,client_update\n",
    "from nv_orga.Eval import evaluate_many_models\n",
    "from nv_orga.NTK import client_compute_eNTK\n",
    "from nv_orga.NTK import compute_eNTK\n",
    "from nv_orga.Scaffold import scaffold_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__définition des hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_client_stage12': 8,\n",
    "    'num_client_stage3': 10,\n",
    "    'seed': 123,\n",
    "    'num_samples_per_client': 5000, #500 de base\n",
    "    'rounds_stage1': 100, #100 de base\n",
    "    'local_epochs_stage1': 1,\n",
    "    'mini_batchsize_stage1': 64,\n",
    "    'local_lr_stage1': 0.1,\n",
    "    'rounds_stage2': 100, #100 de base\n",
    "    'local_steps_stage2': 50,\n",
    "    'local_lr_stage2': 1e-4,\n",
    "    #'weight_decay' : 1e-4,\n",
    "    #'momentum' : 0.9,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__création d'un dossier de sauvegarde pour les modèles successifs du stage 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isExist = os.path.exists('data/ckpt_stage1')\n",
    "if not isExist:\n",
    "   os.makedirs('data/ckpt_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients_stage12 = args[\"num_client_stage12\"]\n",
    "num_clients_stage3 = args[\"num_client_stage3\"]\n",
    "num_rounds_stage1 = args[\"rounds_stage1\"]\n",
    "epochs_stage1 = args[\"local_epochs_stage1\"]\n",
    "batch_size_stage1 = args[\"mini_batchsize_stage1\"]\n",
    "lr_stage1 = args[\"local_lr_stage1\"]\n",
    "#weight_decay = args[\"weight_decay\"]\n",
    "#momentum = args[\"momentum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création des datasets décentralisés (ie non idd)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## load les data MNIST, les transformer en tensor et les normaliser\n",
    "#traindata = datasets.MNIST('data/data_mnist', train=True, download=True,\n",
    "                            #transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                            #transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "## load CIFAR-10 data, transform them into tensors, and normalize\n",
    "traindata = datasets.CIFAR10('data/data_cifar10', train=True, download=True,\n",
    "                            transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]))\n",
    "\n",
    "targets = torch.tensor(traindata.targets)\n",
    "#target_labels = torch.stack([traindata.targets == i for i in range(10)], dim=0).int() # 10 x 60000 (one-hot qui détermine la label correpondant à la ligne)\n",
    "target_labels = torch.stack([targets == i for i in range(10)], dim=0).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_split = []\n",
    "\n",
    "torch.manual_seed(args[\"seed\"]) # pour que les splits soient les mêmes à chaque fois\n",
    "torch.cuda.manual_seed(args[\"seed\"])  # pour que les splits soient les mêmes à chaque fois\n",
    "\n",
    "for i in range(num_clients_stage3):\n",
    "    #index_split = torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0] # on prend les labels 2i et 2i+1\n",
    "    index_split = torch.where(target_labels[0:10].sum(0))[0] # on prend les labels 2i et 2i+1\n",
    "    perm_split = torch.randperm(index_split.size(0)) # on mélange les indices\n",
    "    index_split_subsample = index_split[perm_split[:args[\"num_samples_per_client\"]]] \n",
    "    target_labels_split += [index_split_subsample] # on ajoute à la liste des labels splités\n",
    "\n",
    "#Chacun des 5 clients reçoit num_samples_per_client images d'un des deux labels associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args[\"seed\"]) # pour que les splits soient les mêmes à chaque fois\n",
    "torch.cuda.manual_seed(args[\"seed\"])  # pour que les splits soient les mêmes à chaque fois\n",
    "\n",
    "label_indexes = []\n",
    "for i in range(num_clients_stage3):\n",
    "    indexes = torch.where(target_labels[i])[0]\n",
    "    #permuted_indexes = torch.randperm(len(indexes))\n",
    "    half_length = len(indexes) // 2\n",
    "    indexes_first_half = indexes[:half_length].tolist()\n",
    "    indexes_second_half = indexes[half_length:].tolist()\n",
    "\n",
    "    label_indexes.append(indexes_first_half)\n",
    "    label_indexes.append(indexes_second_half)\n",
    "\n",
    "\n",
    "list_repartition = [[4, 16], [2, 6], [0, 8], [10, 18], [3, 5], [1, 7], [9, 19], [11, 14], [12, 17], [13, 15]]\n",
    "target_labels_split = []\n",
    "for i in range(num_clients_stage3) :\n",
    "        print()\n",
    "        target_labels_split.append(label_indexes[list_repartition[i][0]] + label_indexes[list_repartition[i][1]])\n",
    "#label_indexes[(2*i+1)%20] + label_indexes[(2*i+2)%20]) for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets (subsampled)\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split] # chaque élément contient les images et labels d'un client\n",
    "train_loader = [torch.utils.data.DataLoader(train_subset, batch_size=batch_size_stage1, shuffle=True)\n",
    "                for train_subset in traindata_split] # on crée les dataloader associés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création du dataset global de test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (subsampled)\n",
    "#testdata = datasets.MNIST('data/data_mnist', train=False,\n",
    "#                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "#                                                        transforms.Normalize((0.1307,), (0.3081,))])) # on charge les données de test\n",
    "\n",
    "\n",
    "#torch.manual_seed(args[\"seed\"])\n",
    "#torch.cuda.manual_seed(args[\"seed\"])\n",
    "#perm_split_test = torch.randperm(testdata.targets.shape[0])\n",
    "#testdata_subset = torch.utils.data.Subset(testdata, perm_split_test[:1000])\n",
    "#test_loader = torch.utils.data.DataLoader(testdata_subset, batch_size=batch_size_stage1, shuffle=False) #pas de shuffle pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = datasets.CIFAR10('data/data_cifar10', train=False,\n",
    "                            transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]))\n",
    "\n",
    "\n",
    "# Convertir les cibles en tenseur si nécessaire\n",
    "targets_cifar = torch.tensor(testdata.targets) if isinstance(testdata.targets, list) else testdata.targets\n",
    "\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "torch.cuda.manual_seed(args[\"seed\"])\n",
    "\n",
    "# Création d'un sous-ensemble aléatoire pour CIFAR10\n",
    "perm_split_test = torch.randperm(targets_cifar.shape[0])\n",
    "testdata_subset = torch.utils.data.Subset(testdata, perm_split_test[:1000]) # on prend 1000 images\n",
    "test_loader = torch.utils.data.DataLoader(testdata_subset, batch_size=batch_size_stage1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle de réseau de neurones de base__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "global_model = ResNet18().to(device) #modifié depuis Net().cuda() #Modèle fédéré\n",
    "client_models = [ResNet18().to(device) for _ in range(num_clients_stage3)] #modifié depuis Net().cuda() #Modèles des clients\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=lr_stage1) for model in client_models]\n",
    "#opt = [optim.SGD(model.parameters(), lr=lr_stage1, weight_decay=weight_decay, momentum=momentum) for model in client_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(global_model.state_dict(), 'model_tct_stage1_ResNet18_Cifar10_round' + str(0) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __imple de FedAvg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run TCT-Stage1 (i.e., FedAvg)\n",
    "# for r in range(1, num_rounds_stage1+1):\n",
    "#     # load global weights\n",
    "#     for model in client_models:\n",
    "#         model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "#     # client update\n",
    "#     loss = 0\n",
    "#     for i in range(num_clients_stage12):\n",
    "#         local_loss = client_update(client_models[i], opt[i], train_loader[i], epoch=epochs_stage1)\n",
    "#         print(\"client i : \", i, \" loss : \", local_loss)\n",
    "#         loss += local_loss\n",
    "\n",
    "#     # average params across neighbors\n",
    "#     average_models(global_model, client_models[:num_clients_stage12])\n",
    "\n",
    "#     # evaluate\n",
    "#     test_losses, accuracies = evaluate_many_models(client_models[:num_clients_stage12], test_loader)\n",
    "#     torch.save(client_models[0].state_dict(), 'data/ckpt_stage1/model_tct_stage1.pth')\n",
    "\n",
    "#     print('%d-th round: average train loss %0.3g | average test loss %0.3g | average test acc: %0.3f' % (\n",
    "#         #r, loss.float() / num_clients, test_losses.float().mean(), accuracies.float().mean()))\n",
    "#         r, loss / num_clients_stage12, test_losses.mean(), accuracies.mean()))\n",
    "    \n",
    "#     if r%2 != 0 :\n",
    "#         save_file_name = 'data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round' + str(r) + '_10clients.pth'\n",
    "#         torch.save(global_model.state_dict(), save_file_name)\n",
    "#         print(\"After update round \", r, \" : \", 'model saved as : ', save_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_stage2 = args[\"rounds_stage2\"]\n",
    "batch_size = args[\"num_samples_per_client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "# # Init and load model ckpt\n",
    "# global_model = Net_eNTK() #supprimer .cuda()\n",
    "# global_model = ResNet18() #supprimer .cuda()\n",
    "\n",
    "# global_model.load_state_dict(torch.load(r'./data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round49_8clients.pth'))\n",
    "# #global_model.fc2 = nn.Linear(128, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "\n",
    "# global_model.load_state_dict(torch.load(r'./data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round49_8clients.pth'))\n",
    "# global_model.fc = nn.Linear(512, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "\n",
    "# print('load model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model18 = ResNet18()\n",
    "# model14 = ResNet14()\n",
    "\n",
    "# model18.load_state_dict(torch.load(r'./data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round49_8clients.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfert des poids des couches convolutives et des BatchNorm2d\n",
    "\n",
    "# # Conv1\n",
    "# model14.conv1.weight.data = model18.conv1.weight.data.clone()\n",
    "# model14.bn1.weight.data = model18.bn1.weight.data.clone()\n",
    "# model14.bn1.bias.data = model18.bn1.bias.data.clone()\n",
    "# model14.bn1.running_mean = model18.bn1.running_mean.clone()\n",
    "# model14.bn1.running_var = model18.bn1.running_var.clone()\n",
    "\n",
    "# # Layer1\n",
    "# for i in range(2):\n",
    "#     model14.layer1[i].conv1.weight.data = model18.layer1[i].conv1.weight.data.clone()\n",
    "#     model14.layer1[i].bn1.weight.data = model18.layer1[i].bn1.weight.data.clone()\n",
    "#     model14.layer1[i].bn1.bias.data = model18.layer1[i].bn1.bias.data.clone()\n",
    "#     model14.layer1[i].bn1.running_mean = model18.layer1[i].bn1.running_mean.clone()\n",
    "#     model14.layer1[i].bn1.running_var = model18.layer1[i].bn1.running_var.clone()\n",
    "\n",
    "#     model14.layer1[i].conv2.weight.data = model18.layer1[i].conv2.weight.data.clone()\n",
    "#     model14.layer1[i].bn2.weight.data = model18.layer1[i].bn2.weight.data.clone()\n",
    "#     model14.layer1[i].bn2.bias.data = model18.layer1[i].bn2.bias.data.clone()\n",
    "#     model14.layer1[i].bn2.running_mean = model18.layer1[i].bn2.running_mean.clone()\n",
    "#     model14.layer1[i].bn2.running_var = model18.layer1[i].bn2.running_var.clone()\n",
    "\n",
    "# for i in range(2):\n",
    "#     model14.layer2[i].conv1.weight.data = model18.layer2[i].conv1.weight.data.clone()\n",
    "#     model14.layer2[i].bn1.weight.data = model18.layer2[i].bn1.weight.data.clone()\n",
    "#     model14.layer2[i].bn1.bias.data = model18.layer2[i].bn1.bias.data.clone()\n",
    "#     model14.layer2[i].bn1.running_mean = model18.layer2[i].bn1.running_mean.clone()\n",
    "#     model14.layer2[i].bn1.running_var = model18.layer2[i].bn1.running_var.clone()\n",
    "\n",
    "#     model14.layer2[i].conv2.weight.data = model18.layer2[i].conv2.weight.data.clone()\n",
    "#     model14.layer2[i].bn2.weight.data = model18.layer2[i].bn2.weight.data.clone()\n",
    "#     model14.layer2[i].bn2.bias.data = model18.layer2[i].bn2.bias.data.clone()\n",
    "#     model14.layer2[i].bn2.running_mean = model18.layer2[i].bn2.running_mean.clone()\n",
    "#     model14.layer2[i].bn2.running_var = model18.layer2[i].bn2.running_var.clone()\n",
    "\n",
    "# for i in range(2):\n",
    "#     model14.layer3[i].conv1.weight.data = model18.layer3[i].conv1.weight.data.clone()\n",
    "#     model14.layer3[i].bn1.weight.data = model18.layer3[i].bn1.weight.data.clone()\n",
    "#     model14.layer3[i].bn1.bias.data = model18.layer3[i].bn1.bias.data.clone()\n",
    "#     model14.layer3[i].bn1.running_mean = model18.layer3[i].bn1.running_mean.clone()\n",
    "#     model14.layer3[i].bn1.running_var = model18.layer3[i].bn1.running_var.clone()\n",
    "\n",
    "#     model14.layer3[i].conv2.weight.data = model18.layer3[i].conv2.weight.data.clone()\n",
    "#     model14.layer3[i].bn2.weight.data = model18.layer3[i].bn2.weight.data.clone()\n",
    "#     model14.layer3[i].bn2.bias.data = model18.layer3[i].bn2.bias.data.clone()\n",
    "#     model14.layer3[i].bn2.running_mean = model18.layer3[i].bn2.running_mean.clone()\n",
    "#     model14.layer3[i].bn2.running_var = model18.layer3[i].bn2.running_var.clone()\n",
    "\n",
    "\n",
    "# # Layer2 et Layer3\n",
    "# # Répétez un processus similaire pour layer2 et layer3 de model14 et model18\n",
    "\n",
    "# # Notez que pour les couches avec downsample, vous devrez également copier ces poids si elles existent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model14.fc = nn.Linear(256, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 512->10 à 512->1)\n",
    "# global_model = model14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Compute eNTK v1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# grad_all = []\n",
    "# target_all = []\n",
    "# target_onehot_all = []\n",
    "# for i in range(num_clients_stage3):\n",
    "#     # Modify the batch size in the training data loader to match the model's batch size\n",
    "#     train_loader[i] = torch.utils.data.DataLoader(traindata_split[i], batch_size=batch_size_stage1, shuffle=True)\n",
    "    \n",
    "#     grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i])\n",
    "#     grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "#     target_all.append(copy.deepcopy(target_i).cpu())\n",
    "#     target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "#     del grad_i\n",
    "#     del target_onehot_i\n",
    "#     del target_i\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# grad_eval, target_eval_onehot, target_eval  = client_compute_eNTK(global_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Compute eNTK v2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "# Init and load model ckpt\n",
    "#global_model = Net_eNTK() #supprimer .cuda()\n",
    "global_model = ResNet18() #supprimer .cuda()\n",
    "\n",
    "#global_model.load_state_dict(torch.load(r'data/ckpt_stage1/model_tct_stage1.pth'))\n",
    "#global_model.fc2 = nn.Linear(128, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "\n",
    "filename = './data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round49_8clients.pth' #r'c:/Users/dmgtr/chocoEA/data/baseline_model/model_tct_stage1_ResNet18_Cifar10_round51_10clients.pth'\n",
    "global_model.load_state_dict(torch.load(filename))\n",
    "global_model.fc = nn.Linear(512, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "\n",
    "print('load model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client  0\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  1\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  2\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  3\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  4\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  5\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  6\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  7\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  8\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n",
      "client  9\n",
      "num batch : 79\n",
      " number nan \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_all = []\n",
    "target_all = []\n",
    "target_onehot_all = []\n",
    "for i in range(num_clients_stage3):\n",
    "    # Modify the batch size in the training data loader to match the model's batch size\n",
    "    train_loader[i] = torch.utils.data.DataLoader(traindata_split[i], batch_size=batch_size_stage1, shuffle=True)\n",
    "    print(\"client \", i)\n",
    "\n",
    "    grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i], subsample_size=100000)\n",
    "    print('\\n number nan \\n', torch.isnan(grad_i).sum().item())\n",
    "\n",
    "    grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "    target_all.append(copy.deepcopy(target_i).cpu())\n",
    "    target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "    del grad_i\n",
    "    del target_onehot_i\n",
    "    del target_i\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked torch.Size([10, 5000, 100000])\n",
      "reshaped torch.Size([50000, 100000])\n",
      "mean torch.Size([100000])\n",
      "std torch.Size([100000])\n"
     ]
    }
   ],
   "source": [
    "all_grads_stacked = torch.stack(grad_all)\n",
    "print('stacked', all_grads_stacked.shape)\n",
    "# Redimensionner pour avoir tous les échantillons dans une dimension\n",
    "\n",
    "all_grads_reshaped = all_grads_stacked.view(-1, all_grads_stacked.size(-1))\n",
    "print('reshaped', all_grads_reshaped.shape)\n",
    "\n",
    "# Calculer la moyenne et l'écart-type sur la dimension des échantillons\n",
    "mean_tensor = torch.mean(all_grads_reshaped, dim=0)\n",
    "print('mean', mean_tensor.shape)  # Doit afficher [100000]\n",
    "\n",
    "std_tensor = torch.std(all_grads_reshaped, dim=0)\n",
    "print('std', std_tensor.shape)  # Doit afficher [100000]\n",
    "\n",
    "del all_grads_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31663])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_indexes_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan_indexes_mean torch.Size([0, 1]) tensor([], size=(0, 1), dtype=torch.int64)\n",
      "nan_indexes_std torch.Size([0, 1]) tensor([], size=(0, 1), dtype=torch.int64)\n",
      "zero_indexes_std torch.Size([4, 1]) tensor([[31663],\n",
      "        [36655],\n",
      "        [62560],\n",
      "        [96647]])\n",
      "31663\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "36655\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "62560\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "96647\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "changes\n",
      "nan_indexes_mean torch.Size([0, 1]) tensor([], size=(0, 1), dtype=torch.int64)\n",
      "nan_indexes_std torch.Size([0, 1]) tensor([], size=(0, 1), dtype=torch.int64)\n",
      "zero_indexes_std torch.Size([0, 1]) tensor([], size=(0, 1), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "nan_indexes_mean = torch.nonzero(torch.isnan(mean_tensor))\n",
    "nan_indexes_std = torch.nonzero(torch.isnan(mean_tensor))\n",
    "\n",
    "print('nan_indexes_mean', nan_indexes_mean.shape, nan_indexes_mean)\n",
    "print('nan_indexes_std', nan_indexes_std.shape, nan_indexes_std)\n",
    "\n",
    "zero_indexes_std = torch.nonzero(std_tensor == 0)\n",
    "print('zero_indexes_std', zero_indexes_std.shape, zero_indexes_std)\n",
    "\n",
    "mean_tensor[nan_indexes_mean] = 0\n",
    "std_tensor[nan_indexes_std] = 1\n",
    "std_tensor[zero_indexes_std[:, 0]] = mean_tensor[zero_indexes_std[:, 0]]\n",
    "for index in zero_indexes_std:\n",
    "    std_tensor[index.item()] = 1\n",
    "print('changes')\n",
    "\n",
    "nan_indexes_mean = torch.nonzero(torch.isnan(mean_tensor))\n",
    "nan_indexes_std = torch.nonzero(torch.isnan(mean_tensor))\n",
    "\n",
    "print('nan_indexes_mean', nan_indexes_mean.shape, nan_indexes_mean)\n",
    "print('nan_indexes_std', nan_indexes_std.shape, nan_indexes_std)\n",
    "\n",
    "zero_indexes_std = torch.nonzero(std_tensor == 0)\n",
    "print('zero_indexes_std', zero_indexes_std.shape, zero_indexes_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_clients_stage3):\n",
    "    print(i)\n",
    "    grad_all_i = (grad_all[i] - mean_tensor) / std_tensor\n",
    "    nan_indexes = torch.nonzero(torch.isnan(grad_all_i))\n",
    "    if nan_indexes.sum()==0 :\n",
    "        print('ok', i)\n",
    "        grad_all[i] = grad_all_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batch : 16\r"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "grad_eval, target_eval_onehot, target_eval  = client_compute_eNTK(global_model, test_loader)\n",
    "grad_eval = (grad_eval - mean_tensor) / std_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clients_stage3) :\n",
    "    print(torch.isnan(grad_all[i]).sum().item())\n",
    "print(torch.isnan(grad_eval).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(grad_all, 'c:/Users/dmgtr/chocoEA/data/baseline_model/grad_all_normalized_v2.pth')\n",
    "#torch.save(target_all, 'c:/Users/dmgtr/chocoEA/data/baseline_model/target_all_normalized_v2.pth')\n",
    "#torch.save(target_onehot_all, 'c:/Users/dmgtr/chocoEA/data/baseline_model/target_onehot_all_normalized_v2.pth')\n",
    "#torch.save(grad_eval, 'c:/Users/dmgtr/chocoEA/data/baseline_model/grad_eval_normalized_v2.pth')\n",
    "#torch.save(target_eval, 'c:/Users/dmgtr/chocoEA/data/baseline_model/target_eval_normalized_v2.pth')\n",
    "#torch.save(target_eval_onehot, 'c:/Users/dmgtr/chocoEA/data/baseline_model/target_eval_onehot_normalized_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_all = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/grad_all_normalized.pth')\n",
    "#print('grad_all')\n",
    "#target_all = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/target_all_normalized.pth')\n",
    "#print('target_all')\n",
    "#target_onehot_all = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/target_onehot_all_normalized.pth')\n",
    "#print('target_onehot_all')\n",
    "#grad_eval = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/grad_eval_normalized.pth')\n",
    "#print('grad_eval')\n",
    "#target_eval = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/target_eval_normalized.pth')\n",
    "#print('target_eval')\n",
    "#target_eval_onehot = torch.load('c:/Users/dmgtr/chocoEA/baseline_model/target_eval_onehot_normalized.pth')\n",
    "#print('target_eval_onehot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __run stage 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init linear models\n",
    "theta_global = torch.zeros(100000, 10) #supprimer .cuda()\n",
    "\n",
    "client_thetas = [torch.zeros_like(theta_global) for _ in range(num_clients_stage3)] #supprimer .cuda()\n",
    "client_hi_s = [torch.zeros_like(theta_global) for _ in range(num_clients_stage3)] #supprimer .cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__prendre pour theta initial la valeur des paramètres du modèle -> échec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = args['seed']\n",
    "#subsample_size = 100000\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "#n_params = sum(p.numel() for p in list(global_model.parameters()))\n",
    "#random_index = torch.randperm(n_params)[:subsample_size]\n",
    "#theta_global = torch.cat([param.flatten() for param in list(model.parameters())])[random_index]\n",
    "\n",
    "#theta_global = theta_global.view(-1, 1).repeat(1, 10)\n",
    "#client_thetas = [theta_global for _ in range(num_clients)] #supprimer .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "local gd \n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "local gd \n",
      "num local iter : 16\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dmgtr\\chocoEA\\impl\\TCT-5clients.ipynb Cell 53\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_clients):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(client_thetas[i])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     theta_hat_update, h_i_client_update \u001b[39m=\u001b[39m scaffold_update(grad_all[i],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                           target_all[i],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                           client_thetas[i],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                                           client_hi_s[i],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                                           theta_global,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                                           M\u001b[39m=\u001b[39;49margs[\u001b[39m\"\u001b[39;49m\u001b[39mlocal_steps_stage2\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                                           lr_local\u001b[39m=\u001b[39;49margs[\u001b[39m\"\u001b[39;49m\u001b[39mlocal_lr_stage2\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(theta_hat_update, h_i_client_update)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dmgtr/chocoEA/impl/TCT-5clients.ipynb#Y411sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     client_hi_s[i] \u001b[39m=\u001b[39m h_i_client_update \u001b[39m*\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dmgtr\\chocoEA\\nv_orga\\Scaffold.py:24\u001b[0m, in \u001b[0;36mscaffold_update\u001b[1;34m(grads_data, targets, theta_client, h_i_client_pre, theta_global, M, lr_local)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m local_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(M):\n\u001b[0;32m     23\u001b[0m     theta_hat_local \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr_local \u001b[39m*\u001b[39m ((\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m num_samples) \u001b[39m*\u001b[39m grads_data\u001b[39m.\u001b[39mt() \u001b[39m@\u001b[39m (grads_data \u001b[39m@\u001b[39m theta_hat_local \u001b[39m-\u001b[39m targets_onehot) \u001b[39m-\u001b[39m h_i_client_update)\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnum local iter :\u001b[39m\u001b[39m'\u001b[39m, local_iter, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39mdel\u001b[39;00m targets\n\u001b[0;32m     28\u001b[0m \u001b[39mdel\u001b[39;00m grads_data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage2\n",
    "for round_idx in range(num_rounds_stage2):\n",
    "    theta_list = []\n",
    "    for i in range(num_clients_stage3):\n",
    "        theta_hat_update, h_i_client_update = scaffold_update(grad_all[i],\n",
    "                                                              target_all[i],\n",
    "                                                              client_thetas[i],\n",
    "                                                              client_hi_s[i],\n",
    "                                                              theta_global,\n",
    "                                                              M=args[\"local_steps_stage2\"],\n",
    "                                                              lr_local=args[\"local_lr_stage2\"])\n",
    "        print(theta_hat_update, h_i_client_update)\n",
    "        client_hi_s[i] = h_i_client_update * 1.0\n",
    "        client_thetas[i] = theta_hat_update * 1.0\n",
    "        theta_list.append(theta_hat_update)\n",
    "\n",
    "    # averaging\n",
    "    theta_global = torch.zeros_like(theta_list[0]) #supprimer .cuda()\n",
    "    for theta_idx in range(num_clients_stage3):\n",
    "        theta_global += (1.0 / num_clients_stage3) * theta_list[theta_idx]\n",
    "\n",
    "        # eval on train\n",
    "    for i in range(num_clients_stage3) :\n",
    "        print('Round %d: evaluating client %d :' % (round_idx, i))\n",
    "        #logits_class_train = torch.cat(grad_all) @ theta_global #supprimer .cuda()\n",
    "        \n",
    "        logits_class_train = grad_all[i] @ theta_global #supprimer .cuda()\n",
    "        _, targets_pred_train = logits_class_train.max(1)\n",
    "        \n",
    "        #train_acc = targets_pred_train.eq(torch.cat(target_all)).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "        \n",
    "        train_acc = targets_pred_train.eq(target_all[i]).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "        print('Round %d: client %d training accuracy = :' % (round_idx, i),  train_acc.item())\n",
    "    \n",
    "    # eval on test\n",
    "    print('Round %d: evaluating' % round_idx)\n",
    "    logits_class_test = grad_eval @ theta_global\n",
    "    _, targets_pred_test = logits_class_test.max(1)\n",
    "    test_acc = targets_pred_test.eq(target_eval).sum() / (1.0 * logits_class_test.shape[0]) #supprimer .cuda()\n",
    "    print('Round %d: test accuracy=%0.5g' % (round_idx, test_acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
