{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des librairies nécessaires__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des autres fichiers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Theo/Documents/Dépots Githubs/2023/chocoEA/impl\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Theo/Documents/Dépots Githubs/2023/chocoEA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_orga.Models import Net, Net_eNTK\n",
    "from nv_orga.FedAvg import average_models,client_update\n",
    "from nv_orga.Eval import evaluate_many_models\n",
    "from nv_orga.NTK import client_compute_eNTK\n",
    "from nv_orga.Scaffold import scaffold_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__définition des hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_client_stage1_2': 5,\n",
    "    'num_client_stage3': 5,\n",
    "    'seed': 123,\n",
    "    'num_samples_per_client': 500,\n",
    "    'rounds_stage1': 50, #100 de base\n",
    "    'local_epochs_stage1': 5,\n",
    "    'mini_batchsize_stage1': 64,\n",
    "    'local_lr_stage1': 0.1,\n",
    "    'rounds_stage2': 1, #100 de base\n",
    "    'local_steps_stage2': 100,\n",
    "    'local_lr_stage2': 0.001,\n",
    "    'rounds_stage3': 100,\n",
    "    'local_steps_stage3': 100,\n",
    "    'local_lr_stage3': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__création d'un dossier de sauvegarde pour les modèles successifs du stage 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isExist = os.path.exists('data/ckpt_stage1')\n",
    "if not isExist:\n",
    "   os.makedirs('data/ckpt_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients_stage1_2 = args[\"num_client_stage1_2\"]\n",
    "num_clients_stage3 = args[\"num_client_stage3\"]\n",
    "num_rounds_stage1 = args[\"rounds_stage1\"]\n",
    "\n",
    "epochs_stage1 = args[\"local_epochs_stage1\"]\n",
    "batch_size_stage1 = args[\"mini_batchsize_stage1\"]\n",
    "lr_stage1 = args[\"local_lr_stage1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création des datasets décentralisés (ie non idd)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load les data MNIST, les transformer en tensor et les normaliser\n",
    "traindata = datasets.MNIST('data/data_mnist', train=True, download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)]) # 10 x 60000 (one-hot qui détermine la label correpondant à la ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_split = []\n",
    "torch.manual_seed(args[\"seed\"]) # pour que les splits soient les mêmes à chaque fois\n",
    "torch.cuda.manual_seed(args[\"seed\"])  # pour que les splits soient les mêmes à chaque fois\n",
    "\n",
    "for i in range(num_clients_stage3):\n",
    "    index_split = torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0] # on prend les labels 2i et 2i+1\n",
    "    perm_split = torch.randperm(index_split.size(0)) # on mélange les indices\n",
    "    index_split_subsample = index_split[perm_split[:args[\"num_samples_per_client\"]]] # on prend les 500 premiers\n",
    "    target_labels_split += [index_split_subsample] # on ajoute à la liste des labels splités\n",
    "\n",
    "#Chacun des 5 clients reçoit 500 images d'un des deux labels associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets (subsampled)\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split] # chaque élément contient les images et labels d'un client\n",
    "train_loader = [torch.utils.data.DataLoader(train_subset, batch_size=batch_size_stage1, shuffle=True)\n",
    "                for train_subset in traindata_split] # on crée les dataloader associés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création du dataset global de test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (subsampled)\n",
    "testdata = datasets.MNIST('data/data_mnist', train=False,\n",
    "                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                        transforms.Normalize((0.1307,), (0.3081,))])) # on charge les données de test\n",
    "\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "torch.cuda.manual_seed(args[\"seed\"])\n",
    "perm_split_test = torch.randperm(testdata.targets.shape[0])\n",
    "testdata_subset = torch.utils.data.Subset(testdata, perm_split_test[:1000])\n",
    "test_loader = torch.utils.data.DataLoader(testdata_subset, batch_size=batch_size_stage1, shuffle=False) #pas de shuffle pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle de réseau de neurones de base__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net() #modifié depuis Net().cuda() #Modèle fédéré\n",
    "client_models = [Net() for _ in range(num_clients_stage3)] #modifié depuis Net().cuda() #Modèles des clients\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "opt = [optim.SGD(model.parameters(), lr=lr_stage1) for model in client_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __imple de FedAvg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round: average train loss 0.0963 | average test loss 7.08 | average test acc: 0.197\n",
      "1-th round: average train loss 0.0635 | average test loss 6.91 | average test acc: 0.199\n",
      "2-th round: average train loss 0.0918 | average test loss 7.12 | average test acc: 0.198\n",
      "3-th round: average train loss 0.0236 | average test loss 7.87 | average test acc: 0.199\n",
      "4-th round: average train loss 0.038 | average test loss 7.58 | average test acc: 0.199\n",
      "5-th round: average train loss 0.0228 | average test loss 6.46 | average test acc: 0.199\n",
      "6-th round: average train loss 0.011 | average test loss 6.36 | average test acc: 0.202\n",
      "7-th round: average train loss 0.0322 | average test loss 6.7 | average test acc: 0.203\n",
      "8-th round: average train loss 0.0272 | average test loss 6.18 | average test acc: 0.222\n",
      "9-th round: average train loss 0.00605 | average test loss 6.78 | average test acc: 0.207\n",
      "10-th round: average train loss 0.00486 | average test loss 5.9 | average test acc: 0.234\n",
      "11-th round: average train loss 0.00804 | average test loss 5.33 | average test acc: 0.248\n",
      "12-th round: average train loss 0.00479 | average test loss 5.29 | average test acc: 0.272\n",
      "13-th round: average train loss 0.0132 | average test loss 5.56 | average test acc: 0.271\n",
      "14-th round: average train loss 0.00275 | average test loss 5.23 | average test acc: 0.287\n",
      "15-th round: average train loss 0.000855 | average test loss 5.35 | average test acc: 0.303\n",
      "16-th round: average train loss 0.00162 | average test loss 5.12 | average test acc: 0.298\n",
      "17-th round: average train loss 0.00102 | average test loss 5.08 | average test acc: 0.332\n",
      "18-th round: average train loss 0.000999 | average test loss 5.3 | average test acc: 0.340\n",
      "19-th round: average train loss 0.000932 | average test loss 5.21 | average test acc: 0.346\n",
      "20-th round: average train loss 0.000954 | average test loss 5.79 | average test acc: 0.345\n",
      "21-th round: average train loss 0.0156 | average test loss 6.3 | average test acc: 0.320\n",
      "22-th round: average train loss 0.00167 | average test loss 5.13 | average test acc: 0.283\n",
      "23-th round: average train loss 0.00138 | average test loss 4.8 | average test acc: 0.344\n",
      "24-th round: average train loss 0.000892 | average test loss 5.68 | average test acc: 0.359\n",
      "25-th round: average train loss 0.0207 | average test loss 5.01 | average test acc: 0.354\n",
      "26-th round: average train loss 0.00345 | average test loss 4.85 | average test acc: 0.258\n",
      "27-th round: average train loss 0.00199 | average test loss 4.73 | average test acc: 0.312\n",
      "28-th round: average train loss 0.00182 | average test loss 4.48 | average test acc: 0.345\n",
      "29-th round: average train loss 0.00202 | average test loss 4.41 | average test acc: 0.377\n",
      "30-th round: average train loss 0.00052 | average test loss 4.61 | average test acc: 0.377\n",
      "31-th round: average train loss 0.00194 | average test loss 4.54 | average test acc: 0.398\n",
      "32-th round: average train loss 0.000643 | average test loss 5.33 | average test acc: 0.380\n",
      "33-th round: average train loss 0.00624 | average test loss 5.35 | average test acc: 0.341\n",
      "34-th round: average train loss 0.000927 | average test loss 4.02 | average test acc: 0.338\n",
      "35-th round: average train loss 0.00108 | average test loss 3.93 | average test acc: 0.410\n",
      "36-th round: average train loss 0.000386 | average test loss 4.26 | average test acc: 0.404\n",
      "37-th round: average train loss 0.00079 | average test loss 4.2 | average test acc: 0.444\n",
      "38-th round: average train loss 0.00125 | average test loss 4.32 | average test acc: 0.436\n",
      "39-th round: average train loss 0.000911 | average test loss 4.61 | average test acc: 0.431\n",
      "40-th round: average train loss 0.000382 | average test loss 5.6 | average test acc: 0.454\n",
      "41-th round: average train loss 0.000398 | average test loss 6.1 | average test acc: 0.388\n",
      "42-th round: average train loss 0.00282 | average test loss 4.89 | average test acc: 0.399\n",
      "43-th round: average train loss 0.00134 | average test loss 4.13 | average test acc: 0.338\n",
      "44-th round: average train loss 0.00142 | average test loss 3.61 | average test acc: 0.442\n",
      "45-th round: average train loss 0.000943 | average test loss 3.66 | average test acc: 0.477\n",
      "46-th round: average train loss 0.000992 | average test loss 4.03 | average test acc: 0.487\n",
      "47-th round: average train loss 0.000882 | average test loss 3.81 | average test acc: 0.509\n",
      "48-th round: average train loss 0.000652 | average test loss 4.12 | average test acc: 0.508\n",
      "49-th round: average train loss 0.000844 | average test loss 4.53 | average test acc: 0.495\n"
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage1 (i.e., FedAvg)\n",
    "for r in range(num_rounds_stage1):\n",
    "    # load global weights\n",
    "    for model in client_models[:num_clients_stage1_2]:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients_stage1_2):\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[i], epoch=epochs_stage1)\n",
    "\n",
    "    # average params across neighbors\n",
    "    average_models(global_model, client_models[:num_clients_stage1_2])\n",
    "\n",
    "    # evaluate\n",
    "    test_losses, accuracies = evaluate_many_models(client_models[:num_clients_stage1_2], test_loader)\n",
    "    torch.save(client_models[0].state_dict(), 'data/ckpt_stage1/stage1_20rounds_1workers.pth')\n",
    "\n",
    "    print('%d-th round: average train loss %0.3g | average test loss %0.3g | average test acc: %0.3f' % (\n",
    "    r, loss / num_clients_stage1_2, test_losses.mean(), accuracies.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(client_models[0].state_dict(), 'data/ckpt_stage1/random.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_stage2 = args[\"rounds_stage2\"]\n",
    "batch_size = args[\"num_samples_per_client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "# Init and load model ckpt\n",
    "global_model = Net_eNTK() #supprimer .cuda()\n",
    "global_model.load_state_dict(torch.load('data/ckpt_stage1/stage1_20rounds_1workers.pth'))\n",
    "global_model.fc2 = nn.Linear(128, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "print('load model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Compute eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 654.62it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 731.15it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 729.63it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 567.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_all = []\n",
    "target_all = []\n",
    "target_onehot_all = []\n",
    "for i in range(num_clients_stage1_2):\n",
    "    grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i])\n",
    "    grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "    target_all.append(copy.deepcopy(target_i).cpu())\n",
    "    target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "    del grad_i\n",
    "    del target_onehot_i\n",
    "    del target_i\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 508.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "grad_eval, target_eval_onehot, target_eval  = client_compute_eNTK(global_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __run stage 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/1xnjycd57cjdlfxk8s7k719c0000gn/T/ipykernel_67099/2973467877.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta_global = torch.tensor(theta_global, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "# Init linear models\n",
    "theta_global = torch.zeros(100000, 10) #supprimer .cuda()\n",
    "theta_global = torch.tensor(theta_global, requires_grad=False)\n",
    "client_thetas = [torch.zeros_like(theta_global) for _ in range(num_clients_stage3)] #supprimer .cuda()\n",
    "client_hi_s = [torch.zeros_like(theta_global) for _ in range(num_clients_stage3)] #supprimer .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run TCT-Stage2\n",
    "# for round_idx in range(num_rounds_stage2):\n",
    "#     theta_list = []\n",
    "#     for i in range(num_clients_stage1_2):\n",
    "#         theta_hat_update, h_i_client_update = scaffold_update(grad_all[i],\n",
    "#                                                               target_all[i],\n",
    "#                                                               client_thetas[i],\n",
    "#                                                               client_hi_s[i],\n",
    "#                                                               theta_global,\n",
    "#                                                               M=args[\"local_steps_stage2\"],\n",
    "#                                                               lr_local=args[\"local_lr_stage2\"])\n",
    "#         client_hi_s[i] = h_i_client_update * 1.0\n",
    "#         client_thetas[i] = theta_hat_update * 1.0\n",
    "#         theta_list.append(theta_hat_update)\n",
    "\n",
    "#     # averaging\n",
    "#     theta_global = torch.zeros_like(theta_list[0]) #supprimer .cuda()\n",
    "#     for theta_idx in range(num_clients_stage1_2):\n",
    "#         theta_global += (1.0 / num_clients_stage1_2) * theta_list[theta_idx]\n",
    "\n",
    "#     # eval on train\n",
    "#     logits_class_train = torch.cat(grad_all) @ theta_global #supprimer .cuda()\n",
    "#     _, targets_pred_train = logits_class_train.max(1)\n",
    "#     train_acc = targets_pred_train.eq(torch.cat(target_all)).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "#     # eval on test\n",
    "#     logits_class_test = grad_eval @ theta_global\n",
    "#     _, targets_pred_test = logits_class_test.max(1)\n",
    "#     test_acc = targets_pred_test.eq(target_eval).sum() / (1.0 * logits_class_test.shape[0]) #supprimer .cuda()\n",
    "#     print('Round %d: train accuracy=%0.5g test accuracy=%0.5g' % (round_idx, train_acc.item(), test_acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_rounds_stage3 = args[\"rounds_stage3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 664.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#Compute NTK\n",
    "for i in range(num_clients_stage1_2, num_clients_stage3):\n",
    "    grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i])\n",
    "    grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "    target_all.append(copy.deepcopy(target_i).cpu())\n",
    "    target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "    del grad_i\n",
    "    del target_onehot_i\n",
    "    del target_i\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: train accuracy=0.64688 test accuracy=0.64062\n",
      "Round 1: train accuracy=0.82812 test accuracy=0.875\n",
      "Round 2: train accuracy=0.8625 test accuracy=0.875\n",
      "Round 3: train accuracy=0.86875 test accuracy=0.85938\n",
      "Round 4: train accuracy=0.875 test accuracy=0.875\n",
      "Round 5: train accuracy=0.89375 test accuracy=0.84375\n",
      "Round 6: train accuracy=0.89688 test accuracy=0.84375\n",
      "Round 7: train accuracy=0.90938 test accuracy=0.85938\n",
      "Round 8: train accuracy=0.91875 test accuracy=0.85938\n",
      "Round 9: train accuracy=0.92813 test accuracy=0.85938\n",
      "Round 10: train accuracy=0.94063 test accuracy=0.875\n",
      "Round 11: train accuracy=0.94687 test accuracy=0.875\n",
      "Round 12: train accuracy=0.94687 test accuracy=0.89062\n",
      "Round 13: train accuracy=0.95 test accuracy=0.89062\n",
      "Round 14: train accuracy=0.95 test accuracy=0.89062\n",
      "Round 15: train accuracy=0.95 test accuracy=0.875\n",
      "Round 16: train accuracy=0.95625 test accuracy=0.89062\n",
      "Round 17: train accuracy=0.9625 test accuracy=0.89062\n",
      "Round 18: train accuracy=0.96875 test accuracy=0.89062\n",
      "Round 19: train accuracy=0.96875 test accuracy=0.89062\n",
      "Round 20: train accuracy=0.96875 test accuracy=0.89062\n",
      "Round 21: train accuracy=0.96875 test accuracy=0.89062\n",
      "Round 22: train accuracy=0.975 test accuracy=0.89062\n",
      "Round 23: train accuracy=0.975 test accuracy=0.89062\n",
      "Round 24: train accuracy=0.975 test accuracy=0.89062\n",
      "Round 25: train accuracy=0.975 test accuracy=0.89062\n",
      "Round 26: train accuracy=0.975 test accuracy=0.89062\n",
      "Round 27: train accuracy=0.975 test accuracy=0.90625\n",
      "Round 28: train accuracy=0.97812 test accuracy=0.90625\n",
      "Round 29: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 30: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 31: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 32: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 33: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 34: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 35: train accuracy=0.98125 test accuracy=0.90625\n",
      "Round 36: train accuracy=0.98125 test accuracy=0.92188\n",
      "Round 37: train accuracy=0.98125 test accuracy=0.9375\n",
      "Round 38: train accuracy=0.98438 test accuracy=0.9375\n",
      "Round 39: train accuracy=0.98438 test accuracy=0.9375\n",
      "Round 40: train accuracy=0.9875 test accuracy=0.9375\n",
      "Round 41: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 42: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 43: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 44: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 45: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 46: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 47: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 48: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 49: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 50: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 51: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 52: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 53: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 54: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 55: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 56: train accuracy=0.99063 test accuracy=0.9375\n",
      "Round 57: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 58: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 59: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 60: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 61: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 62: train accuracy=0.99375 test accuracy=0.9375\n",
      "Round 63: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 64: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 65: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 66: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 67: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 68: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 69: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 70: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 71: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 72: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 73: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 74: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 75: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 76: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 77: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 78: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 79: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 80: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 81: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 82: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 83: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 84: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 85: train accuracy=0.99375 test accuracy=0.95312\n",
      "Round 86: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 87: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 88: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 89: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 90: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 91: train accuracy=0.99687 test accuracy=0.95312\n",
      "Round 92: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 93: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 94: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 95: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 96: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 97: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 98: train accuracy=0.99687 test accuracy=0.96875\n",
      "Round 99: train accuracy=0.99687 test accuracy=0.96875\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('./data/dataforplots', exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "file_path = './data/dataforplots/accuracies_5workers.txt'\n",
    "\n",
    "# Run TCT-Stage2\n",
    "accuracies = []\n",
    "for round_idx in range(num_rounds_stage3):\n",
    "    theta_list = []\n",
    "    for i in range(num_clients_stage3):\n",
    "        theta_hat_update, h_i_client_update = scaffold_update(grad_all[i],\n",
    "                                                              target_all[i],\n",
    "                                                              client_thetas[i],\n",
    "                                                              client_hi_s[i],\n",
    "                                                              theta_global,\n",
    "                                                              M=args[\"local_steps_stage2\"],\n",
    "                                                              lr_local=args[\"local_lr_stage2\"])\n",
    "        client_hi_s[i] = h_i_client_update * 1.0\n",
    "        client_thetas[i] = theta_hat_update * 1.0\n",
    "        theta_list.append(theta_hat_update)\n",
    "\n",
    "    # averaging\n",
    "    theta_global = torch.zeros_like(theta_list[0]) #supprimer .cuda()\n",
    "    for theta_idx in range(num_clients_stage3):\n",
    "        theta_global += (1.0 / num_clients_stage3) * theta_list[theta_idx]\n",
    "\n",
    "    # eval on train\n",
    "    logits_class_train = torch.cat(grad_all) @ theta_global #supprimer .cuda()\n",
    "    _, targets_pred_train = logits_class_train.max(1)\n",
    "    train_acc = targets_pred_train.eq(torch.cat(target_all)).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "    # eval on test\n",
    "    logits_class_test = grad_eval @ theta_global\n",
    "    _, targets_pred_test = logits_class_test.max(1)\n",
    "    test_acc = targets_pred_test.eq(target_eval).sum() / (1.0 * logits_class_test.shape[0]) #supprimer .cuda()\n",
    "    print('Round %d: train accuracy=%0.5g test accuracy=%0.5g' % (round_idx, train_acc.item(), test_acc.item()))\n",
    "    accuracies.append(test_acc.item())\n",
    "\n",
    "    # Write the accuracy to the text file\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(f'Round {round_idx}: {test_acc.item()}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
