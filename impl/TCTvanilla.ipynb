{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des librairies nécessaires__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__import des autres fichiers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Theo/Documents/Dépots Githubs/2023/chocoEA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Theo/Documents/Dépots Githubs/2023\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nv_orga.Models import Net, Net_eNTK\n",
    "from nv_orga.FedAvg import average_models,client_update\n",
    "from nv_orga.Eval import evaluate_many_models\n",
    "from nv_orga.NTK import client_compute_eNTK\n",
    "from nv_orga.Scaffold import scaffold_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__définition des hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'num_client': 5,\n",
    "    'seed': 123,\n",
    "    'num_samples_per_client': 500,\n",
    "    'rounds_stage1': 100, #100 de base\n",
    "    'local_epochs_stage1': 5,\n",
    "    'mini_batchsize_stage1': 64,\n",
    "    'local_lr_stage1': 0.1,\n",
    "    'rounds_stage2': 200, #100 de base\n",
    "    'local_steps_stage2': 100,\n",
    "    'local_lr_stage2': 0.001,\n",
    "    'rounds_stage3': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__création d'un dossier de sauvegarde pour les modèles successifs du stage 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "isExist = os.path.exists('data/ckpt_stage1')\n",
    "if not isExist:\n",
    "   os.makedirs('data/ckpt_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = args[\"num_client\"]\n",
    "num_rounds_stage1 = args[\"rounds_stage1\"]\n",
    "\n",
    "epochs_stage1 = args[\"local_epochs_stage1\"]\n",
    "batch_size_stage1 = args[\"mini_batchsize_stage1\"]\n",
    "lr_stage1 = args[\"local_lr_stage1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création des datasets décentralisés (ie non idd)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load les data MNIST, les transformer en tensor et les normaliser\n",
    "traindata = datasets.MNIST('data/data_mnist', train=True, download=True,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)]) # 10 x 60000 (one-hot qui détermine la label correpondant à la ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_split = []\n",
    "torch.manual_seed(args[\"seed\"]) # pour que les splits soient les mêmes à chaque fois\n",
    "torch.cuda.manual_seed(args[\"seed\"])  # pour que les splits soient les mêmes à chaque fois\n",
    "\n",
    "for i in range(num_clients):\n",
    "    index_split = torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0] # on prend les labels 2i et 2i+1\n",
    "    perm_split = torch.randperm(index_split.size(0)) # on mélange les indices\n",
    "    index_split_subsample = index_split[perm_split[:args[\"num_samples_per_client\"]]] # on prend les 500 premiers\n",
    "    target_labels_split += [index_split_subsample] # on ajoute à la liste des labels splités\n",
    "\n",
    "#Chacun des 5 clients reçoit 500 images d'un des deux labels associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets (subsampled)\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split] # chaque élément contient les images et labels d'un client\n",
    "train_loader = [torch.utils.data.DataLoader(train_subset, batch_size=batch_size_stage1, shuffle=True)\n",
    "                for train_subset in traindata_split] # on crée les dataloader associés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __création du dataset global de test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (subsampled)\n",
    "testdata = datasets.MNIST('data/data_mnist', train=False,\n",
    "                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                        transforms.Normalize((0.1307,), (0.3081,))])) # on charge les données de test\n",
    "\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "torch.cuda.manual_seed(args[\"seed\"])\n",
    "perm_split_test = torch.randperm(testdata.targets.shape[0])\n",
    "testdata_subset = torch.utils.data.Subset(testdata, perm_split_test[:1000])\n",
    "test_loader = torch.utils.data.DataLoader(testdata_subset, batch_size=batch_size_stage1, shuffle=False) #pas de shuffle pour le test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle de réseau de neurones de base__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net() #modifié depuis Net().cuda() #Modèle fédéré\n",
    "client_models = [Net() for _ in range(num_clients)] #modifié depuis Net().cuda() #Modèles des clients\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "opt = [optim.SGD(model.parameters(), lr=lr_stage1) for model in client_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __imple de FedAvg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round: average train loss 0.18 | average test loss 6.58 | average test acc: 0.194\n",
      "1-th round: average train loss 0.0922 | average test loss 7.6 | average test acc: 0.192\n",
      "2-th round: average train loss 0.0428 | average test loss 7.51 | average test acc: 0.195\n",
      "3-th round: average train loss 0.0274 | average test loss 7.25 | average test acc: 0.195\n",
      "4-th round: average train loss 0.0339 | average test loss 7.56 | average test acc: 0.196\n",
      "5-th round: average train loss 0.0219 | average test loss 6.34 | average test acc: 0.195\n"
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage1 (i.e., FedAvg)\n",
    "for r in range(num_rounds_stage1):\n",
    "    # load global weights\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_clients):\n",
    "        loss += client_update(client_models[i], opt[i], train_loader[i], epoch=epochs_stage1)\n",
    "\n",
    "    # average params across neighbors\n",
    "    average_models(global_model, client_models)\n",
    "\n",
    "    # evaluate\n",
    "    test_losses, accuracies = evaluate_many_models(client_models, test_loader)\n",
    "    # torch.save(client_models[0].state_dict(), 'data/ckpt_stage1/model_tct_stage1.pth')\n",
    "\n",
    "    print('%d-th round: average train loss %0.3g | average test loss %0.3g | average test acc: %0.3f' % (\n",
    "    r, loss / num_clients, test_losses.mean(), accuracies.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Stage 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __hyperparamètres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds_stage2 = args[\"rounds_stage2\"]\n",
    "batch_size = args[\"num_samples_per_client\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __modèle eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "# Init and load model ckpt\n",
    "global_model = Net_eNTK() #supprimer .cuda()\n",
    "global_model.load_state_dict(torch.load('data/ckpt_stage1/stage1_100rounds_5workers.pth'))\n",
    "global_model.fc2 = nn.Linear(128, 1) #supprimer .cuda() #récupérer une unique sortie ici #supprimer le dernier layer pour le remplacer (passer de 128->10 à 128->1)\n",
    "print('load model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Compute eNTK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 36/64 [00:00<00:00, 354.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 373.20it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 493.31it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 525.38it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 506.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "grad_all = []\n",
    "target_all = []\n",
    "target_onehot_all = []\n",
    "for i in range(num_clients):\n",
    "    grad_i, target_onehot_i, target_i = client_compute_eNTK(global_model, train_loader[i])\n",
    "    grad_all.append(copy.deepcopy(grad_i).cpu())\n",
    "    target_all.append(copy.deepcopy(target_i).cpu())\n",
    "    target_onehot_all.append(copy.deepcopy(target_onehot_i).cpu())\n",
    "    del grad_i\n",
    "    del target_onehot_i\n",
    "    del target_i\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 493.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "grad_eval, target_eval_onehot, target_eval  = client_compute_eNTK(global_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __run stage 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/1xnjycd57cjdlfxk8s7k719c0000gn/T/ipykernel_92887/1872500005.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta_global = torch.tensor(theta_global, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "# Init linear models\n",
    "theta_global = torch.zeros(100000, 10) #supprimer .cuda()\n",
    "theta_global = torch.tensor(theta_global, requires_grad=False)\n",
    "client_thetas = [torch.zeros_like(theta_global) for _ in range(num_clients)] #supprimer .cuda()\n",
    "client_hi_s = [torch.zeros_like(theta_global) for _ in range(num_clients)] #supprimer .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: train accuracy=0.94141 test accuracy=0.73438\n",
      "Round 1: train accuracy=0.96875 test accuracy=0.78125\n",
      "Round 2: train accuracy=0.98047 test accuracy=0.78125\n",
      "Round 3: train accuracy=0.98438 test accuracy=0.78125\n",
      "Round 4: train accuracy=0.98828 test accuracy=0.76562\n",
      "Round 5: train accuracy=0.99219 test accuracy=0.76562\n",
      "Round 6: train accuracy=0.99219 test accuracy=0.76562\n",
      "Round 7: train accuracy=0.99219 test accuracy=0.78125\n",
      "Round 8: train accuracy=0.99609 test accuracy=0.78125\n",
      "Round 9: train accuracy=0.99609 test accuracy=0.78125\n",
      "Round 10: train accuracy=1 test accuracy=0.78125\n",
      "Round 11: train accuracy=1 test accuracy=0.78125\n",
      "Round 12: train accuracy=1 test accuracy=0.78125\n",
      "Round 13: train accuracy=1 test accuracy=0.78125\n",
      "Round 14: train accuracy=1 test accuracy=0.78125\n",
      "Round 15: train accuracy=1 test accuracy=0.78125\n",
      "Round 16: train accuracy=1 test accuracy=0.78125\n",
      "Round 17: train accuracy=1 test accuracy=0.78125\n",
      "Round 18: train accuracy=1 test accuracy=0.78125\n",
      "Round 19: train accuracy=1 test accuracy=0.78125\n",
      "Round 20: train accuracy=1 test accuracy=0.79688\n",
      "Round 21: train accuracy=1 test accuracy=0.79688\n",
      "Round 22: train accuracy=1 test accuracy=0.79688\n",
      "Round 23: train accuracy=1 test accuracy=0.79688\n",
      "Round 24: train accuracy=1 test accuracy=0.79688\n",
      "Round 25: train accuracy=1 test accuracy=0.79688\n",
      "Round 26: train accuracy=1 test accuracy=0.79688\n",
      "Round 27: train accuracy=1 test accuracy=0.79688\n",
      "Round 28: train accuracy=1 test accuracy=0.79688\n",
      "Round 29: train accuracy=1 test accuracy=0.79688\n",
      "Round 30: train accuracy=1 test accuracy=0.79688\n",
      "Round 31: train accuracy=1 test accuracy=0.79688\n",
      "Round 32: train accuracy=1 test accuracy=0.79688\n",
      "Round 33: train accuracy=1 test accuracy=0.79688\n",
      "Round 34: train accuracy=1 test accuracy=0.79688\n",
      "Round 35: train accuracy=1 test accuracy=0.79688\n",
      "Round 36: train accuracy=1 test accuracy=0.79688\n",
      "Round 37: train accuracy=1 test accuracy=0.79688\n",
      "Round 38: train accuracy=1 test accuracy=0.79688\n",
      "Round 39: train accuracy=1 test accuracy=0.79688\n",
      "Round 40: train accuracy=1 test accuracy=0.79688\n",
      "Round 41: train accuracy=1 test accuracy=0.79688\n",
      "Round 42: train accuracy=1 test accuracy=0.79688\n",
      "Round 43: train accuracy=1 test accuracy=0.79688\n",
      "Round 44: train accuracy=1 test accuracy=0.79688\n",
      "Round 45: train accuracy=1 test accuracy=0.79688\n",
      "Round 46: train accuracy=1 test accuracy=0.79688\n",
      "Round 47: train accuracy=1 test accuracy=0.79688\n",
      "Round 48: train accuracy=1 test accuracy=0.79688\n",
      "Round 49: train accuracy=1 test accuracy=0.79688\n",
      "Round 50: train accuracy=1 test accuracy=0.79688\n",
      "Round 51: train accuracy=1 test accuracy=0.79688\n",
      "Round 52: train accuracy=1 test accuracy=0.79688\n",
      "Round 53: train accuracy=1 test accuracy=0.79688\n",
      "Round 54: train accuracy=1 test accuracy=0.79688\n",
      "Round 55: train accuracy=1 test accuracy=0.79688\n",
      "Round 56: train accuracy=1 test accuracy=0.79688\n",
      "Round 57: train accuracy=1 test accuracy=0.79688\n",
      "Round 58: train accuracy=1 test accuracy=0.79688\n",
      "Round 59: train accuracy=1 test accuracy=0.79688\n",
      "Round 60: train accuracy=1 test accuracy=0.79688\n",
      "Round 61: train accuracy=1 test accuracy=0.79688\n",
      "Round 62: train accuracy=1 test accuracy=0.79688\n",
      "Round 63: train accuracy=1 test accuracy=0.79688\n",
      "Round 64: train accuracy=1 test accuracy=0.79688\n",
      "Round 65: train accuracy=1 test accuracy=0.79688\n",
      "Round 66: train accuracy=1 test accuracy=0.79688\n",
      "Round 67: train accuracy=1 test accuracy=0.79688\n",
      "Round 68: train accuracy=1 test accuracy=0.79688\n",
      "Round 69: train accuracy=1 test accuracy=0.79688\n",
      "Round 70: train accuracy=1 test accuracy=0.79688\n",
      "Round 71: train accuracy=1 test accuracy=0.79688\n",
      "Round 72: train accuracy=1 test accuracy=0.79688\n",
      "Round 73: train accuracy=1 test accuracy=0.79688\n",
      "Round 74: train accuracy=1 test accuracy=0.79688\n",
      "Round 75: train accuracy=1 test accuracy=0.79688\n",
      "Round 76: train accuracy=1 test accuracy=0.79688\n",
      "Round 77: train accuracy=1 test accuracy=0.79688\n",
      "Round 78: train accuracy=1 test accuracy=0.79688\n",
      "Round 79: train accuracy=1 test accuracy=0.79688\n",
      "Round 80: train accuracy=1 test accuracy=0.79688\n",
      "Round 81: train accuracy=1 test accuracy=0.79688\n",
      "Round 82: train accuracy=1 test accuracy=0.79688\n",
      "Round 83: train accuracy=1 test accuracy=0.79688\n",
      "Round 84: train accuracy=1 test accuracy=0.79688\n",
      "Round 85: train accuracy=1 test accuracy=0.79688\n",
      "Round 86: train accuracy=1 test accuracy=0.79688\n",
      "Round 87: train accuracy=1 test accuracy=0.79688\n",
      "Round 88: train accuracy=1 test accuracy=0.79688\n",
      "Round 89: train accuracy=1 test accuracy=0.79688\n",
      "Round 90: train accuracy=1 test accuracy=0.79688\n",
      "Round 91: train accuracy=1 test accuracy=0.79688\n",
      "Round 92: train accuracy=1 test accuracy=0.79688\n",
      "Round 93: train accuracy=1 test accuracy=0.79688\n",
      "Round 94: train accuracy=1 test accuracy=0.79688\n",
      "Round 95: train accuracy=1 test accuracy=0.79688\n",
      "Round 96: train accuracy=1 test accuracy=0.79688\n",
      "Round 97: train accuracy=1 test accuracy=0.79688\n",
      "Round 98: train accuracy=1 test accuracy=0.79688\n",
      "Round 99: train accuracy=1 test accuracy=0.79688\n",
      "Round 100: train accuracy=1 test accuracy=0.79688\n",
      "Round 101: train accuracy=1 test accuracy=0.79688\n",
      "Round 102: train accuracy=1 test accuracy=0.79688\n",
      "Round 103: train accuracy=1 test accuracy=0.79688\n",
      "Round 104: train accuracy=1 test accuracy=0.79688\n",
      "Round 105: train accuracy=1 test accuracy=0.79688\n",
      "Round 106: train accuracy=1 test accuracy=0.79688\n",
      "Round 107: train accuracy=1 test accuracy=0.79688\n",
      "Round 108: train accuracy=1 test accuracy=0.79688\n",
      "Round 109: train accuracy=1 test accuracy=0.79688\n",
      "Round 110: train accuracy=1 test accuracy=0.79688\n",
      "Round 111: train accuracy=1 test accuracy=0.79688\n",
      "Round 112: train accuracy=1 test accuracy=0.79688\n",
      "Round 113: train accuracy=1 test accuracy=0.79688\n",
      "Round 114: train accuracy=1 test accuracy=0.79688\n",
      "Round 115: train accuracy=1 test accuracy=0.79688\n",
      "Round 116: train accuracy=1 test accuracy=0.79688\n",
      "Round 117: train accuracy=1 test accuracy=0.79688\n",
      "Round 118: train accuracy=1 test accuracy=0.79688\n",
      "Round 119: train accuracy=1 test accuracy=0.79688\n",
      "Round 120: train accuracy=1 test accuracy=0.79688\n",
      "Round 121: train accuracy=1 test accuracy=0.79688\n",
      "Round 122: train accuracy=1 test accuracy=0.79688\n",
      "Round 123: train accuracy=1 test accuracy=0.79688\n",
      "Round 124: train accuracy=1 test accuracy=0.79688\n",
      "Round 125: train accuracy=1 test accuracy=0.79688\n",
      "Round 126: train accuracy=1 test accuracy=0.79688\n",
      "Round 127: train accuracy=1 test accuracy=0.79688\n",
      "Round 128: train accuracy=1 test accuracy=0.79688\n",
      "Round 129: train accuracy=1 test accuracy=0.79688\n",
      "Round 130: train accuracy=1 test accuracy=0.79688\n",
      "Round 131: train accuracy=1 test accuracy=0.79688\n",
      "Round 132: train accuracy=1 test accuracy=0.79688\n",
      "Round 133: train accuracy=1 test accuracy=0.79688\n",
      "Round 134: train accuracy=1 test accuracy=0.79688\n",
      "Round 135: train accuracy=1 test accuracy=0.79688\n",
      "Round 136: train accuracy=1 test accuracy=0.79688\n",
      "Round 137: train accuracy=1 test accuracy=0.79688\n",
      "Round 138: train accuracy=1 test accuracy=0.79688\n",
      "Round 139: train accuracy=1 test accuracy=0.79688\n",
      "Round 140: train accuracy=1 test accuracy=0.79688\n",
      "Round 141: train accuracy=1 test accuracy=0.79688\n",
      "Round 142: train accuracy=1 test accuracy=0.79688\n",
      "Round 143: train accuracy=1 test accuracy=0.79688\n",
      "Round 144: train accuracy=1 test accuracy=0.79688\n",
      "Round 145: train accuracy=1 test accuracy=0.79688\n",
      "Round 146: train accuracy=1 test accuracy=0.79688\n",
      "Round 147: train accuracy=1 test accuracy=0.79688\n",
      "Round 148: train accuracy=1 test accuracy=0.79688\n",
      "Round 149: train accuracy=1 test accuracy=0.79688\n",
      "Round 150: train accuracy=1 test accuracy=0.79688\n",
      "Round 151: train accuracy=1 test accuracy=0.79688\n",
      "Round 152: train accuracy=1 test accuracy=0.79688\n",
      "Round 153: train accuracy=1 test accuracy=0.79688\n",
      "Round 154: train accuracy=1 test accuracy=0.79688\n",
      "Round 155: train accuracy=1 test accuracy=0.79688\n",
      "Round 156: train accuracy=1 test accuracy=0.79688\n",
      "Round 157: train accuracy=1 test accuracy=0.79688\n",
      "Round 158: train accuracy=1 test accuracy=0.79688\n",
      "Round 159: train accuracy=1 test accuracy=0.79688\n",
      "Round 160: train accuracy=1 test accuracy=0.79688\n",
      "Round 161: train accuracy=1 test accuracy=0.79688\n",
      "Round 162: train accuracy=1 test accuracy=0.79688\n",
      "Round 163: train accuracy=1 test accuracy=0.79688\n",
      "Round 164: train accuracy=1 test accuracy=0.79688\n",
      "Round 165: train accuracy=1 test accuracy=0.79688\n",
      "Round 166: train accuracy=1 test accuracy=0.79688\n",
      "Round 167: train accuracy=1 test accuracy=0.79688\n",
      "Round 168: train accuracy=1 test accuracy=0.79688\n",
      "Round 169: train accuracy=1 test accuracy=0.79688\n",
      "Round 170: train accuracy=1 test accuracy=0.79688\n",
      "Round 171: train accuracy=1 test accuracy=0.79688\n",
      "Round 172: train accuracy=1 test accuracy=0.79688\n",
      "Round 173: train accuracy=1 test accuracy=0.79688\n",
      "Round 174: train accuracy=1 test accuracy=0.79688\n",
      "Round 175: train accuracy=1 test accuracy=0.79688\n",
      "Round 176: train accuracy=1 test accuracy=0.79688\n",
      "Round 177: train accuracy=1 test accuracy=0.79688\n",
      "Round 178: train accuracy=1 test accuracy=0.79688\n",
      "Round 179: train accuracy=1 test accuracy=0.79688\n",
      "Round 180: train accuracy=1 test accuracy=0.79688\n",
      "Round 181: train accuracy=1 test accuracy=0.79688\n",
      "Round 182: train accuracy=1 test accuracy=0.79688\n",
      "Round 183: train accuracy=1 test accuracy=0.79688\n",
      "Round 184: train accuracy=1 test accuracy=0.79688\n",
      "Round 185: train accuracy=1 test accuracy=0.79688\n",
      "Round 186: train accuracy=1 test accuracy=0.79688\n",
      "Round 187: train accuracy=1 test accuracy=0.79688\n",
      "Round 188: train accuracy=1 test accuracy=0.79688\n",
      "Round 189: train accuracy=1 test accuracy=0.79688\n",
      "Round 190: train accuracy=1 test accuracy=0.79688\n",
      "Round 191: train accuracy=1 test accuracy=0.79688\n",
      "Round 192: train accuracy=1 test accuracy=0.79688\n",
      "Round 193: train accuracy=1 test accuracy=0.79688\n",
      "Round 194: train accuracy=1 test accuracy=0.79688\n",
      "Round 195: train accuracy=1 test accuracy=0.79688\n",
      "Round 196: train accuracy=1 test accuracy=0.79688\n",
      "Round 197: train accuracy=1 test accuracy=0.79688\n",
      "Round 198: train accuracy=1 test accuracy=0.79688\n",
      "Round 199: train accuracy=1 test accuracy=0.79688\n"
     ]
    }
   ],
   "source": [
    "# Run TCT-Stage2\n",
    "for round_idx in range(num_rounds_stage2):\n",
    "    theta_list = []\n",
    "    for i in range(num_clients):\n",
    "        theta_hat_update, h_i_client_update = scaffold_update(grad_all[i],\n",
    "                                                              target_all[i],\n",
    "                                                              client_thetas[i],\n",
    "                                                              client_hi_s[i],\n",
    "                                                              theta_global,\n",
    "                                                              M=args[\"local_steps_stage2\"],\n",
    "                                                              lr_local=args[\"local_lr_stage2\"])\n",
    "        client_hi_s[i] = h_i_client_update * 1.0\n",
    "        client_thetas[i] = theta_hat_update * 1.0\n",
    "        theta_list.append(theta_hat_update)\n",
    "\n",
    "    # averaging\n",
    "    theta_global = torch.zeros_like(theta_list[0]) #supprimer .cuda()\n",
    "    for theta_idx in range(num_clients):\n",
    "        theta_global += (1.0 / num_clients) * theta_list[theta_idx]\n",
    "\n",
    "    # eval on train\n",
    "    logits_class_train = torch.cat(grad_all) @ theta_global #supprimer .cuda()\n",
    "    _, targets_pred_train = logits_class_train.max(1)\n",
    "    train_acc = targets_pred_train.eq(torch.cat(target_all)).sum() / (1.0 * logits_class_train.shape[0]) #supprimer .cuda()\n",
    "    # eval on test\n",
    "    logits_class_test = grad_eval @ theta_global\n",
    "    _, targets_pred_test = logits_class_test.max(1)\n",
    "    test_acc = targets_pred_test.eq(target_eval).sum() / (1.0 * logits_class_test.shape[0]) #supprimer .cuda()\n",
    "    print('Round %d: train accuracy=%0.5g test accuracy=%0.5g' % (round_idx, train_acc.item(), test_acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
